diff --git a/clang/include/clang/Basic/CodeGenOptions.def b/clang/include/clang/Basic/CodeGenOptions.def
index c7e01eb12..fd2ff8ee1 100644
--- a/clang/include/clang/Basic/CodeGenOptions.def
+++ b/clang/include/clang/Basic/CodeGenOptions.def
@@ -392,6 +392,9 @@ CODEGENOPT(KeepStaticConsts, 1, 0)
 /// Whether to not follow the AAPCS that enforce at least one read before storing to a volatile bitfield
 CODEGENOPT(ForceAAPCSBitfieldLoad, 1, 0)
 
+/// Mitigate LVI-NULL in switches
+CODEGENOPT(MitigateLVINull, 1, 0)
+
 #undef CODEGENOPT
 #undef ENUM_CODEGENOPT
 #undef VALUE_CODEGENOPT
diff --git a/clang/include/clang/Driver/Options.td b/clang/include/clang/Driver/Options.td
index f818acb39..367d83bed 100644
--- a/clang/include/clang/Driver/Options.td
+++ b/clang/include/clang/Driver/Options.td
@@ -1247,6 +1247,9 @@ def fexperimental_isel : Flag<["-"], "fexperimental-isel">, Group<f_clang_Group>
 def fexperimental_new_pass_manager : Flag<["-"], "fexperimental-new-pass-manager">,
   Group<f_clang_Group>, Flags<[CC1Option]>,
   HelpText<"Enables an experimental new pass manager in LLVM.">;
+def flvi_null : Flag<["-"], "flvi-null">,
+  Group<f_Group>, Flags<[DriverOption,CC1Option]>,
+  HelpText<"Mitigate LVI-NULL in switches.">;
 def fexperimental_strict_floating_point : Flag<["-"], "fexperimental-strict-floating-point">,
   Group<f_clang_Group>, Flags<[CC1Option]>,
   HelpText<"Enables experimental strict floating point in LLVM.">;
diff --git a/clang/lib/CodeGen/BackendUtil.cpp b/clang/lib/CodeGen/BackendUtil.cpp
index dce094067..96f13ccf9 100644
--- a/clang/lib/CodeGen/BackendUtil.cpp
+++ b/clang/lib/CodeGen/BackendUtil.cpp
@@ -77,6 +77,7 @@
 #include "llvm/Transforms/Utils.h"
 #include "llvm/Transforms/Utils/CanonicalizeAliases.h"
 #include "llvm/Transforms/Utils/EntryExitInstrumenter.h"
+#include "llvm/Transforms/Utils/LVIMitigation.h"
 #include "llvm/Transforms/Utils/NameAnonGlobals.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
 #include "llvm/Transforms/Utils/UniqueInternalLinkageNames.h"
@@ -1375,6 +1376,9 @@ void EmitAssemblyHelper::EmitAssemblyWithNewPassManager(
       addCoroutinePassesAtO0(MPM, LangOpts, CodeGenOpts);
       addSanitizersAtO0(MPM, TargetTriple, LangOpts, CodeGenOpts);
     }
+
+    if (CodeGenOpts.MitigateLVINull)
+      MPM.addPass(LVIMitigationPass());
   }
 
   // FIXME: We still use the legacy pass manager to do code generation. We
diff --git a/clang/lib/Driver/ToolChains/Clang.cpp b/clang/lib/Driver/ToolChains/Clang.cpp
index f0a545132..a8b5f0ca0 100644
--- a/clang/lib/Driver/ToolChains/Clang.cpp
+++ b/clang/lib/Driver/ToolChains/Clang.cpp
@@ -5635,6 +5635,10 @@ void Clang::ConstructJob(Compilation &C, const JobAction &JA,
   Args.AddLastArg(CmdArgs, options::OPT_fexperimental_new_pass_manager,
                   options::OPT_fno_experimental_new_pass_manager);
 
+  // Enable mitigating lvi-null in switches and making some loads GS-relative
+  if (Args.hasArg(options::OPT_flvi_null))
+    CmdArgs.push_back("-flvi-null");
+
   ObjCRuntime Runtime = AddObjCRuntimeArgs(Args, Inputs, CmdArgs, rewriteKind);
   RenderObjCOptions(TC, D, RawTriple, Args, Runtime, rewriteKind != RK_None,
                     Input, CmdArgs);
diff --git a/clang/lib/Frontend/CompilerInvocation.cpp b/clang/lib/Frontend/CompilerInvocation.cpp
index 73114c6d7..281dae1cf 100644
--- a/clang/lib/Frontend/CompilerInvocation.cpp
+++ b/clang/lib/Frontend/CompilerInvocation.cpp
@@ -805,6 +805,8 @@ static bool ParseCodeGenArgs(CodeGenOptions &Opts, ArgList &Args, InputKind IK,
   Opts.EmbedSource = Args.hasArg(OPT_gembed_source);
   Opts.ForceDwarfFrameSection = Args.hasArg(OPT_fforce_dwarf_frame);
 
+  Opts.MitigateLVINull = Args.hasArg(OPT_flvi_null);
+
   for (const auto &Arg : Args.getAllArgValues(OPT_fdebug_prefix_map_EQ)) {
     auto Split = StringRef(Arg).split('=');
     Opts.DebugPrefixMap.insert(
diff --git a/llvm/include/llvm/Transforms/Utils/LVIMitigation.h b/llvm/include/llvm/Transforms/Utils/LVIMitigation.h
new file mode 100644
index 000000000..a286518e0
--- /dev/null
+++ b/llvm/include/llvm/Transforms/Utils/LVIMitigation.h
@@ -0,0 +1,29 @@
+//===- LVIMitigation.cpp - LVI-NULL Mitigation Pass ----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass mtigates LVI-NULL in switches by adding fence in the zero or default
+// case.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_TRANSFORMS_UTILS_LVIMITIGATION_H
+#define LLVM_TRANSFORMS_UTILS_LVIMITIGATION_H
+
+#include "llvm/IR/PassManager.h"
+
+namespace llvm {
+
+/// This pass is responsible for mitigating lvi-null in switches
+class LVIMitigationPass : public PassInfoMixin<LVIMitigationPass> {
+public:
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM);
+  StringRef getPassName() const;
+};
+} // end namespace llvm
+
+#endif // LLVM_TRANSFORMS_UTILS_LVIMITIGATION_H
diff --git a/llvm/lib/Passes/PassBuilder.cpp b/llvm/lib/Passes/PassBuilder.cpp
index 4db7bebcb..b43adfd40 100644
--- a/llvm/lib/Passes/PassBuilder.cpp
+++ b/llvm/lib/Passes/PassBuilder.cpp
@@ -186,6 +186,7 @@
 #include "llvm/Transforms/Utils/LibCallsShrinkWrap.h"
 #include "llvm/Transforms/Utils/LoopSimplify.h"
 #include "llvm/Transforms/Utils/LowerInvoke.h"
+#include "llvm/Transforms/Utils/LVIMitigation.h"
 #include "llvm/Transforms/Utils/Mem2Reg.h"
 #include "llvm/Transforms/Utils/NameAnonGlobals.h"
 #include "llvm/Transforms/Utils/SymbolRewriter.h"
diff --git a/llvm/lib/Target/X86/CMakeLists.txt b/llvm/lib/Target/X86/CMakeLists.txt
index 729934b29..5bd561e85 100644
--- a/llvm/lib/Target/X86/CMakeLists.txt
+++ b/llvm/lib/Target/X86/CMakeLists.txt
@@ -74,6 +74,7 @@ set(sources
   X86WinAllocaExpander.cpp
   X86WinEHState.cpp
   X86InsertWait.cpp
+  X86LVINullMitigation.cpp
   )
 
 add_llvm_target(X86CodeGen ${sources})
diff --git a/llvm/lib/Target/X86/X86.h b/llvm/lib/Target/X86/X86.h
index 91ba4e3d0..43bdb3a13 100644
--- a/llvm/lib/Target/X86/X86.h
+++ b/llvm/lib/Target/X86/X86.h
@@ -144,6 +144,7 @@ FunctionPass *createX86LoadValueInjectionLoadHardeningPass();
 FunctionPass *createX86LoadValueInjectionRetHardeningPass();
 FunctionPass *createX86SpeculativeLoadHardeningPass();
 FunctionPass *createX86SpeculativeExecutionSideEffectSuppression();
+FunctionPass *createX86LoadValueInjectionNULLHardeningPass();
 
 void initializeEvexToVexInstPassPass(PassRegistry &);
 void initializeFixupBWInstPassPass(PassRegistry &);
@@ -166,6 +167,7 @@ void initializeX86OptimizeLEAPassPass(PassRegistry &);
 void initializeX86PartialReductionPass(PassRegistry &);
 void initializeX86SpeculativeLoadHardeningPassPass(PassRegistry &);
 void initializeX86SpeculativeExecutionSideEffectSuppressionPass(PassRegistry &);
+void initializeX86LoadValueInjectionNULLHardeningPassPass(PassRegistry &);
 
 namespace X86AS {
 enum : unsigned {
diff --git a/llvm/lib/Target/X86/X86LVINullMitigation.cpp b/llvm/lib/Target/X86/X86LVINullMitigation.cpp
new file mode 100644
index 000000000..58dd89a28
--- /dev/null
+++ b/llvm/lib/Target/X86/X86LVINullMitigation.cpp
@@ -0,0 +1,897 @@
+//===-- X86LoadValueInjectionNULLHardening.cpp - LVI NULL hardening for x86 --==//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+///
+/// Description: This pass is responsible for replacing instructions that perform
+/// implicit loads such as push, pop, ret, and different call instructions. First, it replaces
+/// every "call" instruction with the following sequence:
+/// ```
+/// lea 8(rip), <scratch-reg>
+/// sub 8, rsp
+/// mov <scratch-reg>, (rsp)
+/// jmp <call-target>
+/// ```
+/// Different call instructions, for instance PCrel calls or calls where the target
+/// is in memory are replaced with different code snippets.
+///
+/// Second, it replaces every `ret` instruction with the sequence:
+/// ```
+/// mov (rsp), <scratch-reg>
+/// jmp *<scratch-reg>
+/// ```
+/// where `<scratch-reg>` is some available scratch register, according to the
+/// calling convention of the function being mitigated.
+/// This is an adaption of Intels' LVI mitigation.
+///
+/// Third, it replaces all push sequences with the following:
+/// ```
+/// sub 0x8, rsp
+/// mov <target-reg>, 0x0(rsp)
+/// ```
+///
+/// Fourth: it replaces all pop sequences with the following:
+/// ```
+/// mov 0x0(rsp), <target-reg>
+/// add 0x8, rsp
+/// ```
+/// Finally, it takes care of loads that have not been made GS-relative in the frontend
+/// by changing the respective addressing.
+///
+//===----------------------------------------------------------------------===//
+
+#include "X86.h"
+#include "X86InstrBuilder.h"
+#include "X86Subtarget.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/MC/MCCodeEmitter.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Support/Debug.h"
+#include <bitset>
+#include <string>
+
+using namespace llvm;
+
+#define PASS_KEY "lvi-null"
+#define DEBUG_TYPE PASS_KEY
+
+#define MIB_ADD_MEM_SYMBOL(_symbol)            \
+    addReg(X86::RIP)                /*base*/   \
+        .addImm(1)                  /*scale*/  \
+        .addReg(X86::NoRegister)    /*index*/  \
+        .addExternalSymbol(_symbol) /*symbol*/ \
+        .addReg(X86::NoRegister)    /*segment*/
+
+#define MIB_ADD_MBB_ADDRESS(_mbb)              \
+    addReg(X86::RIP)                /*base*/   \
+        .addImm(1)                  /*scale*/  \
+        .addReg(X86::NoRegister)    /*index*/  \
+        .addMBB(_mbb)               /*symbol*/ \
+        .addReg(X86::NoRegister)    /*segment*/
+
+
+static cl::opt<bool> EnableLVINullMitigation(
+    "x86-enable-lvi-null", cl::NotHidden,
+    cl::desc("X86: Enable LVI-NULL mitigations in the backend."), cl::init(false));
+
+static cl::opt<bool> EnableExperimentalInlineAsmFix(
+    "x86-lvi-null-fix-inline-asm", cl::NotHidden,
+    cl::desc("X86: LVI-NULL mitigations try to fix inline assembly."), cl::init(false));
+
+STATISTIC(NumCalls, "Number of calls modified for LVI-NULL mitigation");
+STATISTIC(NumPush, "Number of push instructions modified for LVI-NULL mitigation");
+STATISTIC(NumPop, "Number of pop instructions modified for LVI-NULL mitigation");
+STATISTIC(NumRet, "Number of ret instructions modified for LVI-NULL mitigation");
+STATISTIC(NumMemOps, "Number of memory operands modified for LVI-NULL mitigation");
+STATISTIC(NumFunctionsConsidered, "Number of functions analyzed");
+STATISTIC(NumFunctionsMitigated, "Number of functions for which mitigations were deployed");
+
+namespace {
+
+class X86LoadValueInjectionNULLHardeningPass : public MachineFunctionPass {
+public:
+  X86LoadValueInjectionNULLHardeningPass() : MachineFunctionPass(ID) {}
+  StringRef getPassName() const override {
+    return "X86 Load Value Injection (LVI) NULL-Hardening";
+  }
+  bool runOnMachineFunction(MachineFunction &MF) override;
+
+  static char ID;
+};
+
+} // end anonymous namespace
+
+char X86LoadValueInjectionNULLHardeningPass::ID = 0;
+
+// Helper function to add ModR/M references for [Seg: Reg + Offset] memory
+// accesses
+static const MachineInstrBuilder& addSegmentedMem(const MachineInstrBuilder &MIB, Register Seg, Register Reg, int Offset = 0) {
+  return MIB.addReg(Reg).addImm(1).addReg(0).addImm(Offset).addReg(Seg);
+}
+
+// check if the EFLAGS register is used 
+static bool checkEFLAGSLive(MachineInstr *MI) {
+  if (MI->killsRegister(X86::EFLAGS))
+    return false;
+
+  // The EFLAGS operand of MI might be missing a kill marker.
+  // Figure out whether EFLAGS operand should LIVE after MI instruction.
+  MachineBasicBlock *BB = MI->getParent();
+  MachineBasicBlock::iterator ItrMI = MI;
+
+  // Scan forward through BB for a use/def of EFLAGS.
+  for (auto I = std::next(ItrMI), E = BB->end(); I != E; ++I) {
+    if (I->readsRegister(X86::EFLAGS))
+      return true;
+    if (I->definesRegister(X86::EFLAGS))
+      return false;
+  }
+
+  // We hit the end of the block, check whether EFLAGS is live into a successor.
+  for (auto I = BB->succ_begin(), E = BB->succ_end(); I != E; ++I) {
+    if ((*I)->isLiveIn(X86::EFLAGS))
+      return true;
+  }
+
+  return false;
+}
+
+// split a basic block into two, this is needed for jumps and leas to determine instruction addresses
+MachineBasicBlock* sliceMBBAt(MachineBasicBlock::iterator MI) {
+    MachineBasicBlock *       MBB = MI->getParent();
+    MachineFunction *         MF  = MBB->getParent();
+    MachineFunction::iterator MFI = ++(MBB->getIterator());
+
+    MachineBasicBlock *sinkMBB = MF->CreateMachineBasicBlock(MBB->getBasicBlock());
+
+    MF->insert(MFI, sinkMBB);
+
+    sinkMBB->splice(sinkMBB->begin(), MBB, std::next(MI), MBB->end());
+    sinkMBB->transferSuccessorsAndUpdatePHIs(MBB);
+    sinkMBB->setHasAddressTaken();
+    sinkMBB->setLabelMustBeEmitted();
+
+    // If the EFLAGS register isn't dead in the terminator, then claim that it's
+    // live into the sink and copy blocks.
+    // this is freaking important otherwise some labels are optimized out and we get the
+    // weirdest backend errors
+    if (checkEFLAGSLive(&*MI)) {
+      sinkMBB->addLiveIn(X86::EFLAGS);
+    }
+
+    MBB->addSuccessor(sinkMBB);
+
+    return sinkMBB;
+}
+
+
+/* convert:
+  ret
+  ---
+  mov %gs:(%rsp), %rcx
+  add $8, %rsp
+  jmp *%rcx
+*/
+bool eliminateRet(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating ret instructions\n");
+
+  // we are executing a return so we use RCX as clobber
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // first we move the return location into a clobber register
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64rm), X86::RCX), X86::GS, X86::RSP, 0);
+  
+  // we adjust the stack
+  BuildMI(MBB, MBB.end(), debug_location, TII->get(X86::ADD64ri8), X86::RSP)
+    .addReg(X86::RSP)
+    .addImm(8);
+
+  // we jump to the location in the clobber register
+  BuildMI(MBB, MBB.end(), debug_location, TII->get(X86::JMP64r))
+    .addReg(X86::RCX);
+
+  NumRet++;
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  push %reg
+  ---
+  sub $8, %rsp
+  mov %reg, %gs:0(%rsp)
+*/
+bool eliminatePushRegister(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  Register used_reg = MI->getOperand(0).getReg();
+
+  LLVM_DEBUG(dbgs() << "Push uses register: "<< MBB.getParent()->getSubtarget<X86Subtarget>().getRegisterInfo()->getRegAsmName(used_reg) << "\n");
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+  
+  if (used_reg == X86::RSP) {
+    addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, -8).addReg(used_reg);
+    BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri32), X86::RSP).addReg(X86::RSP).addImm(8);
+  } else {
+    // first, we adjust the stack
+    BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri32), X86::RSP).addReg(X86::RSP).addImm(8);
+    
+    // then we save our used register on the stack
+    addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, 0).addReg(used_reg);
+  }
+  NumPush++;
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  push "mem"
+  ---
+  mov %rax, %gs:-8(%rsp)
+  mov "mem", %rax
+  xchg %rax, %gs:-8(%rsp)
+  sub $8, %rsp             // do not subtract befire reading the mem op (mem op can be on the stack!)
+*/
+bool eliminatePushMemory(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+
+  // extract the memory operand
+  auto base         = MI->getOperand(X86::AddrBaseReg).getReg();
+  auto scale        = MI->getOperand(X86::AddrScaleAmt).getImm();
+  auto index        = MI->getOperand(X86::AddrIndexReg).getReg();
+  auto displacement = MI->getOperand(X86::AddrDisp);
+  auto segment      = MI->getOperand(X86::AddrSegmentReg).getReg();
+
+  // change the segment register here if not already gs
+  if (segment == X86::NoRegister) {
+    segment = X86::GS;
+  }
+  if (segment == X86::GS && base == X86::RIP) {
+    base = X86::NoRegister;
+  }
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+  
+  Register used_reg = X86::RAX;
+
+  // save clobber register!
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, -8)
+    .addReg(used_reg);
+
+  // store memory operand inside clobber register
+  BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64rm))
+    .addReg(used_reg)
+    .addReg(base)         // at X86::AddrBaseReg
+    .addImm(scale)        // at X86::AddrScaleAmt
+    .addReg(index)        // at X86::AddrIndexReg
+    .add(displacement) // at X86::AddrDisp
+    .addReg(segment);     // at X86::AddrSegmentReg
+
+  // swap clobber register with memory operand
+  BuildMI(MBB, MI, debug_location, TII->get(X86::XCHG64rm))
+    .addReg(used_reg)
+    .addReg(used_reg)
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(-8)
+    .addReg(X86::GS);
+
+  // dont move the sub ... the memory operand can be itself on the stack and that would change the stack!
+  // fadjust the stack
+  BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri32), X86::RSP).addReg(X86::RSP).addImm(8);
+
+  NumPush++;
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  pop %reg
+  ---
+  mov %gs:0(%rsp), %reg
+  add $8, %rsp
+*/
+bool eliminatePop(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  Register used_reg = MI->getOperand(0).getReg();
+  LLVM_DEBUG(dbgs() << "Pop uses register: "<< MBB.getParent()->getSubtarget<X86Subtarget>().getRegisterInfo()->getRegAsmName(used_reg) << "\n");
+  
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // first, we restore our value from the stack
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64rm), used_reg), X86::GS, X86::RSP, 0);
+
+  // only adjust RSP if we not currently poped RSP
+  if (used_reg != X86::RSP) {
+    // then we adjust the stack
+    BuildMI(MBB, MI, debug_location, TII->get(X86::ADD64ri8), X86::RSP).addReg(X86::RSP).addImm(8);
+  }
+
+  NumPop++;
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  call symbol_like
+  ---
+  lea 1f(%rip), %r11
+  sub $8, %rsp
+  mob %r11, %gs:0(%rsp)
+  jmp symbol_like
+1:
+*/
+bool eliminateCallPCRel(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating PCRel call instructions\n");
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // the call target
+  // please dear "insert god figure" make this thing outlive the MI we erase later! 
+  MachineOperand call_dest = MI->getOperand(0);
+  
+  // slice the MBB so we don't need to calculate the lea offset by hand
+  MachineBasicBlock *sinkMBB = sliceMBBAt(MI);
+
+  // first, we load the rip and off the block after
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), X86::R11).MIB_ADD_MBB_ADDRESS(sinkMBB);
+
+  // then we adjust the stack
+  BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri8), X86::RSP).addReg(X86::RSP).addImm(8);
+
+  // then we move the return address we got in the first step onto the stack
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, 0).addReg(X86::R11);
+
+  // finally, we can jump to our function instead of calling it
+  BuildMI(MBB, MI, debug_location, TII->get(X86::JMP_1)).add(call_dest);
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+    
+  NumCalls++;
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  call *%reg
+  ---
+  lea __ImageBase(%rip), %clobber
+  add %clobber, %reg
+  lea 1f(%rip), %clobber
+  sub $8, %rsp
+  mob %clobber, %gs:0(%rsp)
+  jmp %reg
+1:
+*/
+bool eliminateCallRegister(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating register call instructions\n");
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // register used in call
+  auto reg = MI->getOperand(0).getReg();
+
+  // we use R11 or RAX as clobber register
+  unsigned clobber_reg = reg != X86::R11 ? X86::R11 : X86::RAX;
+
+  // convert address
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), clobber_reg).MIB_ADD_MEM_SYMBOL("__ImageBase");
+  BuildMI(MBB, MI, debug_location, TII->get(X86::ADD64rr), reg).addReg(reg).addReg(clobber_reg);
+
+  // slice the MBB so we don't need to calculate the lea offset by hand
+  MachineBasicBlock *sinkMBB = sliceMBBAt(MI);
+
+  // first, we load the rip and off the block after
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), clobber_reg).MIB_ADD_MBB_ADDRESS(sinkMBB);
+
+  // then we adjust the stack
+  BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri8), X86::RSP).addReg(X86::RSP).addImm(8);
+
+  // then we move the return address we got in the first step onto the stack
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, 0).addReg(clobber_reg);
+  
+  // finally, we can jump to our function instead of calling it
+  BuildMI(MBB, MI, debug_location, TII->get(X86::JMP64r)).addReg(reg);
+
+  NumCalls++;
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  call *"mem"
+  ---
+  lea __ImageBase(%rip), %clobber
+  add "mem", %clobber
+  sub $8, %rsp
+  mov %clobber, %gs:0(%rsp)
+  lea 1f(%rip), %clobber
+  xchg %clobber, %gs:0(%rsp)
+  jmp %clobber
+1:
+*/
+bool eliminateCallMemory(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating memory call instructions\n");
+
+  // extract the memory operand
+  auto base         = MI->getOperand(X86::AddrBaseReg).getReg();
+  auto scale        = MI->getOperand(X86::AddrScaleAmt).getImm();
+  auto index        = MI->getOperand(X86::AddrIndexReg).getReg();
+  auto displacement = MI->getOperand(X86::AddrDisp);
+  auto segment      = MI->getOperand(X86::AddrSegmentReg).getReg();
+
+  // change the segment register here if not already gs
+  if (segment == X86::NoRegister) {
+    segment = X86::GS;
+  }
+  if (segment == X86::GS && base == X86::RIP) {
+    base = X86::NoRegister;
+  }
+
+  // we use R11, R10 or RAX as clobber register
+  unsigned clobber_reg;
+  // check if neither base nor index is the clobber register
+  if (base != X86::R11 && index != X86::R11) {
+    clobber_reg = X86::R11;
+  } else if (base != X86::RAX && index != X86::RAX) {
+    clobber_reg = X86::RAX;
+  } else {
+    clobber_reg = X86::R10;
+  }
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // slice the MBB so we don't need to calculate the lea offset by hand
+  MachineBasicBlock *sinkMBB = sliceMBBAt(MI);
+
+
+  // get image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), clobber_reg)
+    .MIB_ADD_MEM_SYMBOL("__ImageBase");
+
+  // add mem op to image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::ADD64rm))
+      .addReg(clobber_reg)
+      .addReg(clobber_reg)
+      .addReg(base)         // at X86::AddrBaseReg
+      .addImm(scale)        // at X86::AddrScaleAmt
+      .addReg(index)        // at X86::AddrIndexReg
+      .add(displacement) // at X86::AddrDisp
+      .addReg(segment);     // at X86::AddrSegmentReg
+
+  // adjust the stack after we have loaded the memory operand since the mem operand can istelf be on the stack
+  BuildMI(MBB, MI, debug_location, TII->get(X86::SUB64ri8), X86::RSP).addReg(X86::RSP)
+    .addImm(8);
+
+  // store the call address at the return address for now
+  addSegmentedMem(BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr)), X86::GS, X86::RSP, 0)
+    .addReg(clobber_reg);
+
+  // load the return address into clobber reg
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), clobber_reg)
+    .MIB_ADD_MBB_ADDRESS(sinkMBB);
+
+  // xchange the return address with the call address
+  BuildMI(MBB, MI, debug_location, TII->get(X86::XCHG64rm))
+    .addReg(clobber_reg)
+    .addReg(clobber_reg)
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(0)
+    .addReg(X86::GS);
+
+  // finally, we can jump to our function instead of calling it
+  BuildMI(MBB, MI, debug_location, TII->get(X86::JMP64r))
+    .addReg(clobber_reg);
+
+  NumCalls++;
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+/* convert:
+  jmp *%reg
+  ---
+  mov %reg, %gs:-4096(%rsp)    // red zone
+  lea __ImageBase(%rip), %reg
+  add %gs:-4096(%rsp), %reg
+  xchg %reg, %gs:-4096(%rsp)   // we need to restore the original value of reg!
+  jmp %gs:-4096(%rsp)
+1:
+  comment: this is tricky since the jmp instruction has no defined clobber register in the abi
+  therefore we use the red zone as temporal storage. One alternativ would be a distinct location inside the TCB and address
+  that over the fs segment
+*/
+bool eliminateJumpRegister(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating jump register instructions\n");
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // register used in call
+  auto reg = MI->getOperand(0).getReg();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // save used_reg on the read zone
+  BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr))
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(-4096)
+    .addReg(X86::GS)
+    .addReg(reg);
+
+  // get image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), reg)
+    .MIB_ADD_MEM_SYMBOL("__ImageBase");
+
+  // add mem op to image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::ADD64rm))
+    .addReg(reg)
+    .addReg(reg)
+    .addReg(X86::RSP)         // at X86::AddrBaseReg
+    .addImm(1)                // at X86::AddrScaleAmt
+    .addReg(X86::NoRegister)  // at X86::AddrIndexReg
+    .addImm(-4096)            // at X86::AddrDisp
+    .addReg(X86::GS);         // at X86::AddrSegmentReg
+
+  // xchange saved old reg_with reg
+  BuildMI(MBB, MI, debug_location, TII->get(X86::XCHG64rm))
+    .addReg(reg)
+    .addReg(reg)
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(-4096)
+    .addReg(X86::GS);
+
+  // the real address is now on the stack! jump to it! ... we can zero this but since we are inside the enclave we cant execute 0
+  BuildMI(MBB, MI, debug_location, TII->get(X86::JMP64m))
+    .addReg(X86::RSP)         // at X86::AddrBaseReg
+    .addImm(1)                // at X86::AddrScaleAmt
+    .addReg(X86::NoRegister)  // at X86::AddrIndexReg
+    .addImm(-4096)            // at X86::AddrDisp
+    .addReg(X86::GS);         // at X86::AddrSegmentReg
+
+  // adjust the iterator
+  MI--;
+  
+  return true;
+}
+
+
+/* convert:
+  jmp *"mem"
+  ---
+  mov %clobber, %gs:-4096(%rsp)    // red zone
+  lea __ImageBase(%rip), %clobber
+  add "mem", %clobber 
+  xchg %gs:-4096(%rsp), %clobber
+  jmp *%gs:-4096(%rsp)
+1:
+  comment: this is tricky since the jmp instruction has no defined clobber register in the abi
+  therefore we use the red zone as temporal storage. One alternativ would be a distinct location inside the TCB and address
+  that over the fs segment
+*/
+bool eliminateJumpMemory(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  LLVM_DEBUG(dbgs() << "I'm eliminating jump memory instructions\n");
+
+  // extract the memory operand
+  auto base         = MI->getOperand(X86::AddrBaseReg).getReg();
+  auto scale        = MI->getOperand(X86::AddrScaleAmt).getImm();
+  auto index        = MI->getOperand(X86::AddrIndexReg).getReg();
+  auto displacement = MI->getOperand(X86::AddrDisp);
+  auto segment      = MI->getOperand(X86::AddrSegmentReg).getReg();
+
+  // change the segment register here if not already gs
+  if (segment == X86::NoRegister) {
+    segment = X86::GS;
+  }
+  if (segment == X86::GS && base == X86::RIP) {
+    base = X86::NoRegister;
+  }
+
+  unsigned used_reg = X86::R11;
+
+  // copy the debug location for better dbg information
+  auto debug_location = MI->getDebugLoc();
+
+  // erase the instruction pointed by MI ... MI now points to the instruction after the one erased
+  MI = MBB.erase_instr(&(*MI));
+
+  // save used_reg on the read zone
+  BuildMI(MBB, MI, debug_location, TII->get(X86::MOV64mr))
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(-4096)
+    .addReg(X86::GS)
+    .addReg(used_reg);
+
+  // get image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::LEA64r), used_reg)
+    .MIB_ADD_MEM_SYMBOL("__ImageBase");
+
+  // add mem op to image base
+  BuildMI(MBB, MI, debug_location, TII->get(X86::ADD64rm))
+    .addReg(used_reg)
+    .addReg(used_reg)
+    .addReg(base)         // at X86::AddrBaseReg
+    .addImm(scale)        // at X86::AddrScaleAmt
+    .addReg(index)        // at X86::AddrIndexReg
+    .add(displacement)    // at X86::AddrDisp
+    .addReg(segment);     // at X86::AddrSegmentReg
+
+  // xchange saved old reg_with reg
+  BuildMI(MBB, MI, debug_location, TII->get(X86::XCHG64rm))
+    .addReg(used_reg)
+    .addReg(used_reg)
+    .addReg(X86::RSP)
+    .addImm(1)
+    .addReg(X86::NoRegister)
+    .addImm(-4096)
+    .addReg(X86::GS);
+
+  // the real address is now on the stack! jump to it! ... we can zero this but since we are inside the enclave we cant execute 0
+  BuildMI(MBB, MI, debug_location, TII->get(X86::JMP64m))
+    .addReg(X86::RSP)         // at X86::AddrBaseReg
+    .addImm(1)                // at X86::AddrScaleAmt
+    .addReg(X86::NoRegister)  // at X86::AddrIndexReg
+    .addImm(-4096)            // at X86::AddrDisp
+    .addReg(X86::GS);         // at X86::AddrSegmentReg
+
+  // adjust the iterator
+  MI--;
+
+  return true;
+}
+
+bool fixInlineAssembly(MachineBasicBlock &MBB, const X86InstrInfo *TII, MachineBasicBlock::iterator &MI) {
+  if (!EnableExperimentalInlineAsmFix) {
+    return false;
+  }
+  
+  LLVM_DEBUG(dbgs() << "I'm eliminating inline asm instructions\n");
+
+  // copy pase from somewhere, to get the asm string
+  // Count the number of register definitions to find the asm string.
+  unsigned NumDefs = 0;
+  for (; MI->getOperand(NumDefs).isReg() && MI->getOperand(NumDefs).isDef();
+      ++NumDefs)
+  assert(NumDefs != MI->getNumOperands()-2 && "No asm string?");
+  assert(MI->getOperand(NumDefs).isSymbol() && "No asm string?");
+
+  std::string str{ MI->getOperand(NumDefs).getSymbolName() };
+
+  using namespace std::string_literals;
+
+  auto debug_location = MI->getDebugLoc();
+
+  if (str == "push %rsp"s) {
+    MI = MBB.erase_instr(&(*MI));
+    BuildMI(MBB, MI, debug_location, TII->get(X86::PUSH64r)).addReg(X86::RSP);
+    MI--;
+    MI--;
+    return true;
+
+  } else if (str == "pop %rsp"s) {
+    MI = MBB.erase_instr(&(*MI));
+    BuildMI(MBB, MI, debug_location, TII->get(X86::POP64r)).addReg(X86::RSP);
+    MI--;
+    MI--;
+    return true;
+  } else if (str == "ret"s) {
+    MI = MBB.erase_instr(&(*MI));
+    BuildMI(MBB, MI, debug_location, TII->get(X86::RETQ));
+    MI--;
+    MI--;
+    return true;
+  }
+
+  return false;
+}
+
+bool mitigateLVI(const X86Subtarget *Subtarget, const Function &F, MachineFunction &MF) {
+  const X86RegisterInfo *TRI = Subtarget->getRegisterInfo();
+  const X86InstrInfo *TII = Subtarget->getInstrInfo();
+
+  bool Modified = false;
+
+  // generate code emitter to determine the real size of instructions
+  auto CodeEmitter = createX86MCCodeEmitter(*TII, *TRI, MF.getContext());
+
+  // for each machine basic block
+  for (auto &MBB : MF) {
+    
+    if (MBB.empty())
+      continue;
+
+    // for each machine instruction
+    for(auto MI=MBB.begin(); MI!=MBB.end(); ++MI) {
+
+      // check the op code
+      switch(MI->getOpcode()) {
+        // RETURN
+        case X86::RETQ:
+          Modified |= eliminateRet(MBB, TII, MI);
+          break;
+
+        // PUSHES
+        case X86::PUSH64r:
+        case X86::PUSH32r:
+          Modified |= eliminatePushRegister(MBB, TII, MI);
+          break;
+        
+        case X86::PUSH64rmm:
+          Modified |= eliminatePushMemory(MBB, TII, MI);
+          break;
+
+        // POPS
+        case X86::POP64r:
+        case X86::POP32r:
+          Modified |= eliminatePop(MBB, TII, MI);
+          break;
+
+        // CALLS
+        case X86::CALL64pcrel32:
+          Modified |= eliminateCallPCRel(MBB, TII, MI);
+          break;
+        case X86::CALL64r:
+          Modified |= eliminateCallRegister(MBB, TII, MI);
+          break;
+        case X86::CALL64m:
+          Modified |= eliminateCallMemory(MBB, TII, MI);
+          break;
+
+        // JUMPS
+        case X86::TAILJMPr:
+        case X86::TAILJMPr64:
+        case X86::JMP64r:
+          Modified |= eliminateJumpRegister(MBB, TII, MI);
+          break;
+
+        case X86::TAILJMPm:
+        case X86::TAILJMPm64:
+        case X86::JMP64m:
+          Modified |= eliminateJumpMemory(MBB, TII, MI);
+          break;
+        
+        // for the POCs we fix a subset of inline assembly statements
+        // this subset is not complete and left for future work
+        case TargetOpcode::INLINEASM:
+          Modified |= fixInlineAssembly(MBB, TII, MI);
+          break;
+
+        // Everything else which uses a memory operand
+        default: {
+          const MCInstrDesc &Desc = MI->getDesc();
+          
+          // check if the operation uses a memory operand
+          int mem_op_index = X86II::getMemoryOperandNo(Desc.TSFlags);
+          if (mem_op_index < 0) {
+            break;
+          }
+          // the index of the memory operand can have a offset
+          mem_op_index += X86II::getOperandBias(Desc);
+
+          // calculate the index of the segment register in the memory operand
+          unsigned base_op_index = mem_op_index + X86::AddrBaseReg;
+          unsigned segment_op_index = mem_op_index + X86::AddrSegmentReg;
+
+          // sanity check .. should not happen
+          if (segment_op_index >= MI->getNumOperands()) {
+            llvm::errs() << "Memory operand does not have allocated segment register!\n";
+            break;
+          }
+
+          // base and segment register
+          auto base    = MI->getOperand(base_op_index).getReg(); 
+          auto segment = MI->getOperand(segment_op_index).getReg();
+
+          // only change segment register if the segment is not already set and the 
+          // memory operand does not use realtive addressing
+          if (/*base != X86::RIP &&*/ segment == X86::NoRegister) {
+            MI->getOperand(segment_op_index).ChangeToRegister(X86::GS, false);
+            Modified = true;
+            if ( base == X86::RIP ) {
+              MI->getOperand(base_op_index).ChangeToRegister(X86::NoRegister, false);
+            }
+          }
+          
+          break;
+        }
+      } // end of switch
+    } // end of for each MO
+  } // end of for each MBB
+
+  delete CodeEmitter;
+
+  return Modified;
+}
+
+bool X86LoadValueInjectionNULLHardeningPass::runOnMachineFunction(
+    MachineFunction &MF) {
+  if(!EnableLVINullMitigation)
+    return false;
+  LLVM_DEBUG(dbgs() << "***** " << getPassName() << " : " << MF.getName()
+                    << " *****\n");
+  const X86Subtarget *Subtarget = &MF.getSubtarget<X86Subtarget>();
+  if (!Subtarget->is64Bit())
+    return false; // FIXME: support 32-bit
+
+  // Don't skip functions with the "optnone" attr but participate in opt-bisect.
+  const Function &F = MF.getFunction();
+  if (!F.hasOptNone() && skipFunction(F))
+    return false;
+  ++NumFunctionsConsidered;
+
+  bool Modified = mitigateLVI(Subtarget, F, MF);
+
+  if (Modified)
+    ++NumFunctionsMitigated;
+  return Modified;
+}
+
+INITIALIZE_PASS(X86LoadValueInjectionNULLHardeningPass, PASS_KEY,
+                "X86 LVI NULL hardener", false, false)
+
+FunctionPass *llvm::createX86LoadValueInjectionNULLHardeningPass() {
+  return new X86LoadValueInjectionNULLHardeningPass();
+}
diff --git a/llvm/lib/Target/X86/X86TargetMachine.cpp b/llvm/lib/Target/X86/X86TargetMachine.cpp
index 7344116e1..63cc55122 100644
--- a/llvm/lib/Target/X86/X86TargetMachine.cpp
+++ b/llvm/lib/Target/X86/X86TargetMachine.cpp
@@ -89,6 +89,7 @@ extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeX86Target() {
   initializeX86LoadValueInjectionRetHardeningPassPass(PR);
   initializeX86OptimizeLEAPassPass(PR);
   initializeX86PartialReductionPass(PR);
+  initializeX86LoadValueInjectionNULLHardeningPassPass(PR);
 }
 
 static std::unique_ptr<TargetLoweringObjectFile> createTLOF(const Triple &TT) {
@@ -552,6 +553,8 @@ void X86PassConfig::addPreEmitPass2() {
   if (TT.isOSWindows())
     addPass(createCFGuardLongjmpPass());
   addPass(createX86LoadValueInjectionRetHardeningPass());
+
+  addPass(createX86LoadValueInjectionNULLHardeningPass());
 }
 
 std::unique_ptr<CSEConfigBase> X86PassConfig::getCSEConfig() const {
diff --git a/llvm/lib/Transforms/Utils/CMakeLists.txt b/llvm/lib/Transforms/Utils/CMakeLists.txt
index 5c26767c1..043c6b552 100644
--- a/llvm/lib/Transforms/Utils/CMakeLists.txt
+++ b/llvm/lib/Transforms/Utils/CMakeLists.txt
@@ -46,6 +46,7 @@ add_llvm_component_library(LLVMTransformUtils
   LowerInvoke.cpp
   LowerMemIntrinsics.cpp
   LowerSwitch.cpp
+  LVIMitigation.cpp
   Mem2Reg.cpp
   MetaRenamer.cpp
   MisExpect.cpp
diff --git a/llvm/lib/Transforms/Utils/LVIMitigation.cpp b/llvm/lib/Transforms/Utils/LVIMitigation.cpp
new file mode 100644
index 000000000..a2054afeb
--- /dev/null
+++ b/llvm/lib/Transforms/Utils/LVIMitigation.cpp
@@ -0,0 +1,138 @@
+//===- LVIMitigation.cpp - LVI-NULL Mitigation Pass ----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This pass first mtigates LVI-NULL in switches by adding a fence in the zero or default
+// case.
+//
+// In a second step it makes all LoadInst and all GlobalVariables relative to the GS segment.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/Transforms/Utils/LVIMitigation.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/LLVMContext.h"
+#include "llvm/IR/Module.h"
+#include "llvm/IR/Type.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "llvm/Transforms/Utils.h"
+#include "llvm/Transforms/Utils/BasicBlockUtils.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/ADT/Statistic.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "lvi-null-ir"
+
+STATISTIC(NumSwitchesConsidered, "Number of switches considered modified for LVI-NULL mitigation");
+STATISTIC(NumSwitchesModified, "Number of switches modified modified for LVI-NULL mitigation");
+STATISTIC(NumZero, "Number of zero cases modified for LVI-NULL mitigation");
+STATISTIC(NumDefault, "Number of default cases modified for LVI-NULL mitigation");
+STATISTIC(NumLoadsModified, "Number of loads modified for LVI-NULL mitigation");
+STATISTIC(NumStoresModified, "Number of store modified for LVI-NULL mitigation");
+STATISTIC(NumGlobalsModified, "Number of globals modified for LVI-NULL mitigation");
+
+StringRef LVIMitigationPass::getPassName() const {
+    return "LVI-NULL mitigation pass";
+}
+
+static bool mitigateLVINullInSwitch(SwitchInst *SI) {
+  IRBuilder<> Builder(&*(SI->getParent()));
+
+  // Iterate over all cases to check if the switch has a case for the value 0
+  bool contains_zero_case = false;
+  int zero_case_index;
+  for(auto _case : SI->cases()) {
+    if(_case.getCaseValue()->getZExtValue() == 0) {
+      zero_case_index = _case.getCaseIndex();
+      contains_zero_case = true;
+      break;
+    }
+  }
+
+  if(contains_zero_case) {
+    BasicBlock *zero_case = SI->getSuccessor(zero_case_index+1);
+    if(isa<FenceInst>(*zero_case->getFirstInsertionPt()))
+      return false;
+
+    new FenceInst(Builder.getContext(), AtomicOrdering::SequentiallyConsistent, SyncScope::System, &*(zero_case->getFirstInsertionPt()));
+    NumZero++;
+    return true;
+  } else {
+    // we get the default case of the switch, if one exists this is simply its basic block, if none exists than it is the basic block that gets executed after the switch
+    BasicBlock *default_case = SI->getDefaultDest();
+    if(isa<FenceInst>(*default_case->getFirstInsertionPt()))
+      return false;
+
+    // we create a new zero case basic block
+    BasicBlock *zero_case = BasicBlock::Create(Builder.getContext(), "lvi", SI->getFunction());
+    // we add our fence instruction to our newly created basic block as its first instruction
+    FenceInst *fence = new FenceInst(Builder.getContext(), AtomicOrdering::SequentiallyConsistent, SyncScope::System, &*(zero_case->getFirstInsertionPt()));
+    // we now create an unconditional branch to the default case
+    BranchInst *branch = BranchInst::Create(default_case, zero_case);
+    // finally, we add the zero case to the branch
+    SI->addCase(Builder.getInt32(0), zero_case);
+    return true;
+  }
+  return false;
+}
+
+static bool mitigateLVINull(Module &M) {
+  bool Modified = false;
+  for(Function &F : M) {
+    for (BasicBlock &BB : F) {
+      for (Instruction &I : BB) {
+        if(auto SI = dyn_cast<SwitchInst>(&I)) {
+          NumSwitchesConsidered++;
+          if(mitigateLVINullInSwitch(SI)) {
+            Modified = true;
+            NumSwitchesModified++;
+          }
+        }
+
+        // make all loads GS-relative if not already
+        /*if(auto LI = dyn_cast<LoadInst>(&I)) {
+          auto type = LI->getPointerOperandType();
+          auto operand = LI->getPointerOperand();
+          if(dyn_cast<GlobalVariable>(operand))
+            continue;
+          if(type) {
+            auto new_type = dyn_cast<PointerType>(type)->getElementType()->getPointerTo(256);
+            LI->getPointerOperand()->mutateType(new_type);
+            Modified = true;
+            NumLoadsModified++;
+          }
+        }*/
+      }
+    }
+  }
+
+  // make all global variables GS-realtive
+  /*for(auto &global : M.getGlobalList()) {
+    auto new_type = global.getType()->getPointerTo(256);
+    global.mutateType(new_type);
+    Modified = true;
+    NumGlobalsModified++;
+  }*/
+  return Modified;
+}
+
+PreservedAnalyses LVIMitigationPass::run(Module &M,
+                                        ModuleAnalysisManager &AM) {
+  LLVM_DEBUG(dbgs() << "***** " << getPassName() << " : " << M.getName()
+                    << " *****\n");
+  if(mitigateLVINull(M))
+    return PreservedAnalyses::none();
+  else
+    return PreservedAnalyses::all();
+}
